{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peternagy/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/peternagy/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import RMSprop\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pl\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I have created this notebook to observe the task with binary classification. This way it was easier to find the most \n",
    "promising features and algorithm to go with.\n",
    "I have found this (http://cs229.stanford.edu/proj2015/328_report.pdf) \"paper\" claiming that they have reached ~0.69\n",
    "accuracy with RandomForest on the same dataset.\n",
    "I couldn't reproduce their result, not even with their best features. More suprisingly I didn't find their best features the best at all.\n",
    "With a fully connected Neural Network on binary classification my best result was ~0.63 accuracy.\n",
    "'''\n",
    "\n",
    "def data():\n",
    "    cols = [' title_sentiment_polarity', ' n_tokens_title', ' data_channel_is_lifestyle', ' data_channel_is_entertainment', ' data_channel_is_bus', ' data_channel_is_socmed', ' data_channel_is_tech', ' data_channel_is_world', ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday', ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday', ' weekday_is_sunday', ' is_weekend', ' global_sentiment_polarity', ' shares']\n",
    "    raw_data = pd.read_csv('data/OnlineNewsPopularity.csv', usecols=cols)\n",
    "    \n",
    "    def categorizeBinaryShares(shares):\n",
    "        if shares <= 1400:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    raw_data['label'] = raw_data[' shares'].apply(categorizeBinaryShares)\n",
    "    raw_data['label'].value_counts()\n",
    "    raw_data.drop([' shares'], axis=1, inplace=True)\n",
    "    \n",
    "    y = raw_data['label']\n",
    "    raw_data.drop(['label'], axis=1, inplace=True)\n",
    "    \n",
    "    raw_data.fillna(\"\", inplace=True)\n",
    "    raw_data = raw_data.reindex()\n",
    "    \n",
    "    def constructBestFeatures(df):\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        if 'url' in raw_data.keys():\n",
    "            raw_data.drop(['url'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        classifier = DecisionTreeClassifier()\n",
    "        classifier.fit(raw_data, y)\n",
    "\n",
    "\n",
    "        importances = []\n",
    "        for name, importance in zip(raw_data.columns, classifier.feature_importances_):\n",
    "            importances.append((name, importance))\n",
    "\n",
    "        num_cols = len(raw_data.columns)\n",
    "        importances_sorted = sorted(importances, key=lambda x: x[1])\n",
    "        importances_sorted\n",
    "        cols = []\n",
    "        for imp_tupl in importances_sorted[-12:]:\n",
    "            cols.append(imp_tupl[0])\n",
    "        return cols\n",
    "\n",
    "    cols = constructBestFeatures(raw_data)\n",
    "    print(cols)\n",
    "    best_features = raw_data[cols]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(best_features, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Flatten, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import RMSprop\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as pl\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.tree import DecisionTreeClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [32,64, 128 ,256, 512, 1024]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dense_1': hp.choice('Dense_1', [32,64, 128 ,256, 512, 1024]),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),\n",
      "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: cols = [' title_sentiment_polarity', ' n_tokens_title', ' data_channel_is_lifestyle', ' data_channel_is_entertainment', ' data_channel_is_bus', ' data_channel_is_socmed', ' data_channel_is_tech', ' data_channel_is_world', ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday', ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday', ' weekday_is_sunday', ' is_weekend', ' global_sentiment_polarity', ' shares']\n",
      "   3: raw_data = pd.read_csv('data/OnlineNewsPopularity.csv', usecols=cols)\n",
      "   4: \n",
      "   5: def categorizeBinaryShares(shares):\n",
      "   6:     if shares <= 1400:\n",
      "   7:         return 0\n",
      "   8:     else:\n",
      "   9:         return 1\n",
      "  10: \n",
      "  11: raw_data['label'] = raw_data[' shares'].apply(categorizeBinaryShares)\n",
      "  12: raw_data['label'].value_counts()\n",
      "  13: raw_data.drop([' shares'], axis=1, inplace=True)\n",
      "  14: \n",
      "  15: y = raw_data['label']\n",
      "  16: raw_data.drop(['label'], axis=1, inplace=True)\n",
      "  17: \n",
      "  18: raw_data.fillna(\"\", inplace=True)\n",
      "  19: raw_data = raw_data.reindex()\n",
      "  20: \n",
      "  21: def constructBestFeatures(df):\n",
      "  22:     from sklearn.tree import DecisionTreeClassifier\n",
      "  23:     if 'url' in raw_data.keys():\n",
      "  24:         raw_data.drop(['url'], axis=1, inplace=True)\n",
      "  25: \n",
      "  26: \n",
      "  27:     classifier = DecisionTreeClassifier()\n",
      "  28:     classifier.fit(raw_data, y)\n",
      "  29: \n",
      "  30: \n",
      "  31:     importances = []\n",
      "  32:     for name, importance in zip(raw_data.columns, classifier.feature_importances_):\n",
      "  33:         importances.append((name, importance))\n",
      "  34: \n",
      "  35:     num_cols = len(raw_data.columns)\n",
      "  36:     importances_sorted = sorted(importances, key=lambda x: x[1])\n",
      "  37:     importances_sorted\n",
      "  38:     cols = []\n",
      "  39:     for imp_tupl in importances_sorted[-12:]:\n",
      "  40:         cols.append(imp_tupl[0])\n",
      "  41:     return cols\n",
      "  42: \n",
      "  43: cols = constructBestFeatures(raw_data)\n",
      "  44: print(cols)\n",
      "  45: best_features = raw_data[cols]\n",
      "  46: \n",
      "  47: X_train, X_test, y_train, y_test = train_test_split(best_features, y, test_size=0.2, random_state=42)\n",
      "  48: \n",
      "  49: \n",
      "  50: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "  1: def keras_fmin_fnct(space):\n",
      "  2: \n",
      "  3:     print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
      "  4:     model = Sequential()\n",
      "  5:     model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
      "  6:     model.add(Dense(space['Dense'], activation='relu'))\n",
      "  7:     model.add(Dropout(space['Dropout']))\n",
      "  8:     model.add(Dense(space['Dense_1'], activation='relu'))\n",
      "  9:     model.add(Dropout(space['Dropout_1']))\n",
      " 10: \n",
      " 11:     model.add(Dense(1, activation='sigmoid'))\n",
      " 12:     model.compile(optimizer=space['optimizer'],\n",
      " 13:                   loss='binary_crossentropy',\n",
      " 14:                   metrics=['accuracy'])\n",
      " 15:     model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=space['batch_size'], validation_data=(np.array(X_test), np.array(y_test)), callbacks=[EarlyStopping(monitor='val_loss', mode='auto')])\n",
      " 16:     score, acc = model.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
      " 17:     print('Test accuracy:', acc)\n",
      " 18:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      " 19: \n",
      "[' data_channel_is_bus', ' data_channel_is_tech', ' weekday_is_monday', ' weekday_is_wednesday', ' weekday_is_tuesday', ' weekday_is_thursday', ' is_weekend', ' data_channel_is_world', ' data_channel_is_entertainment', ' n_tokens_title', ' title_sentiment_polarity', ' global_sentiment_polarity']\n",
      "(31715, 12) (31715,) (7929, 12) (7929,)\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 1s - loss: 0.7105 - acc: 0.5074 - val_loss: 0.6890 - val_acc: 0.5830\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 1s - loss: 0.6901 - acc: 0.5281 - val_loss: 0.6848 - val_acc: 0.5819\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 1s - loss: 0.6876 - acc: 0.5449 - val_loss: 0.6823 - val_acc: 0.6211\n",
      "Epoch 4/10\n",
      "31715/31715 [==============================] - 1s - loss: 0.6858 - acc: 0.5496 - val_loss: 0.6787 - val_acc: 0.6244\n",
      "Epoch 5/10\n",
      "31715/31715 [==============================] - 1s - loss: 0.6829 - acc: 0.5595 - val_loss: 0.6752 - val_acc: 0.6165\n",
      "Epoch 6/10\n",
      "31715/31715 [==============================] - 1s - loss: 0.6808 - acc: 0.5663 - val_loss: 0.6724 - val_acc: 0.6147\n",
      "Epoch 7/10\n",
      "31715/31715 [==============================] - 1s - loss: 0.6798 - acc: 0.5692 - val_loss: 0.6734 - val_acc: 0.6127\n",
      "Test accuracy: 0.6126876025471114\n",
      "(31715, 12) (31715,) (7929, 12) (7929,)\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.7567 - acc: 0.5003 - val_loss: 0.6917 - val_acc: 0.5139\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6928 - acc: 0.5161 - val_loss: 0.6868 - val_acc: 0.5561\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6888 - acc: 0.5398 - val_loss: 0.6837 - val_acc: 0.6074\n",
      "Epoch 4/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6851 - acc: 0.5474 - val_loss: 0.6791 - val_acc: 0.5756\n",
      "Epoch 5/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6829 - acc: 0.5587 - val_loss: 0.6716 - val_acc: 0.5933\n",
      "Epoch 6/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6827 - acc: 0.5640 - val_loss: 0.6737 - val_acc: 0.5904\n",
      "Test accuracy: 0.5903644848251751\n",
      "(31715, 12) (31715,) (7929, 12) (7929,)\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.7144 - acc: 0.5012 - val_loss: 0.6929 - val_acc: 0.5139\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6932 - acc: 0.5054 - val_loss: 0.6925 - val_acc: 0.5195\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6934 - acc: 0.5047 - val_loss: 0.6929 - val_acc: 0.5139\n",
      "Test accuracy: 0.513936183637231\n",
      "(31715, 12) (31715,) (7929, 12) (7929,)\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 1s - loss: 0.6916 - acc: 0.5475 - val_loss: 0.6707 - val_acc: 0.6206\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 1s - loss: 0.6735 - acc: 0.5853 - val_loss: 0.6682 - val_acc: 0.6185\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 1s - loss: 0.6698 - acc: 0.5962 - val_loss: 0.6680 - val_acc: 0.6175\n",
      "Epoch 4/10\n",
      "31715/31715 [==============================] - 1s - loss: 0.6676 - acc: 0.6056 - val_loss: 0.6695 - val_acc: 0.6242\n",
      "Test accuracy: 0.6241644596013134\n",
      "(31715, 12) (31715,) (7929, 12) (7929,)\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 1s - loss: 0.6839 - acc: 0.5700 - val_loss: 0.6662 - val_acc: 0.6074\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31715/31715 [==============================] - 0s - loss: 0.6655 - acc: 0.6015 - val_loss: 0.6591 - val_acc: 0.6211\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6620 - acc: 0.6099 - val_loss: 0.6583 - val_acc: 0.6243\n",
      "Epoch 4/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6617 - acc: 0.6111 - val_loss: 0.6792 - val_acc: 0.6051\n",
      "Test accuracy: 0.6051204438948634\n",
      "(31715, 12) (31715,) (7929, 12) (7929,)\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.7206 - acc: 0.5058 - val_loss: 0.6902 - val_acc: 0.5534\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6898 - acc: 0.5276 - val_loss: 0.6872 - val_acc: 0.5683\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6891 - acc: 0.5316 - val_loss: 0.6859 - val_acc: 0.5736\n",
      "Epoch 4/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6883 - acc: 0.5354 - val_loss: 0.6847 - val_acc: 0.5616\n",
      "Epoch 5/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6870 - acc: 0.5414 - val_loss: 0.6837 - val_acc: 0.5836\n",
      "Epoch 6/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6855 - acc: 0.5458 - val_loss: 0.6813 - val_acc: 0.5776\n",
      "Epoch 7/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6844 - acc: 0.5538 - val_loss: 0.6802 - val_acc: 0.5902\n",
      "Epoch 8/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6836 - acc: 0.5585 - val_loss: 0.6798 - val_acc: 0.5726\n",
      "Epoch 9/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6824 - acc: 0.5612 - val_loss: 0.6765 - val_acc: 0.5868\n",
      "Epoch 10/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6815 - acc: 0.5638 - val_loss: 0.6770 - val_acc: 0.5756\n",
      "Test accuracy: 0.5756085257404523\n",
      "(31715, 12) (31715,) (7929, 12) (7929,)\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 3s - loss: 0.6921 - acc: 0.5222 - val_loss: 0.6820 - val_acc: 0.5708\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 2s - loss: 0.6727 - acc: 0.5920 - val_loss: 0.6596 - val_acc: 0.6162\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 2s - loss: 0.6675 - acc: 0.6141 - val_loss: 0.6574 - val_acc: 0.6209\n",
      "Epoch 4/10\n",
      "31715/31715 [==============================] - 2s - loss: 0.6649 - acc: 0.6169 - val_loss: 0.6585 - val_acc: 0.6232\n",
      "Test accuracy: 0.6231555051303839\n",
      "(31715, 12) (31715,) (7929, 12) (7929,)\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 1s - loss: 0.6982 - acc: 0.5240 - val_loss: 0.6832 - val_acc: 0.5617\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6872 - acc: 0.5432 - val_loss: 0.6790 - val_acc: 0.5889\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6806 - acc: 0.5694 - val_loss: 0.6740 - val_acc: 0.6055\n",
      "Epoch 4/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6750 - acc: 0.5818 - val_loss: 0.6682 - val_acc: 0.5801\n",
      "Epoch 5/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6720 - acc: 0.5887 - val_loss: 0.6624 - val_acc: 0.6006\n",
      "Epoch 6/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6692 - acc: 0.5924 - val_loss: 0.6619 - val_acc: 0.6032\n",
      "Epoch 7/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6663 - acc: 0.5983 - val_loss: 0.6702 - val_acc: 0.6039\n",
      "Test accuracy: 0.6038592508062015\n",
      "(31715, 12) (31715,) (7929, 12) (7929,)\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6871 - acc: 0.5613 - val_loss: 0.6646 - val_acc: 0.5957\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6698 - acc: 0.5974 - val_loss: 0.6616 - val_acc: 0.6050\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6655 - acc: 0.6089 - val_loss: 0.6574 - val_acc: 0.6112\n",
      "Epoch 4/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6644 - acc: 0.6088 - val_loss: 0.6550 - val_acc: 0.6244\n",
      "Epoch 5/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6636 - acc: 0.6117 - val_loss: 0.6536 - val_acc: 0.6282\n",
      "Epoch 6/10\n",
      "31715/31715 [==============================] - 0s - loss: 0.6624 - acc: 0.6137 - val_loss: 0.6545 - val_acc: 0.6249\n",
      "Test accuracy: 0.6249211754545105\n",
      "(31715, 12) (31715,) (7929, 12) (7929,)\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 2s - loss: 0.6672 - acc: 0.5954 - val_loss: 0.6587 - val_acc: 0.5992\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 1s - loss: 0.6599 - acc: 0.6151 - val_loss: 0.6554 - val_acc: 0.6209\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 1s - loss: 0.6584 - acc: 0.6177 - val_loss: 0.6586 - val_acc: 0.6203\n",
      "Test accuracy: 0.6202547610264616\n",
      "[' data_channel_is_bus', ' data_channel_is_tech', ' weekday_is_monday', ' weekday_is_wednesday', ' weekday_is_tuesday', ' is_weekend', ' weekday_is_thursday', ' data_channel_is_world', ' data_channel_is_entertainment', ' n_tokens_title', ' title_sentiment_polarity', ' global_sentiment_polarity']\n",
      "Evalutation of best performing model:\n",
      "5856/7929 [=====================>........] - ETA: 0s[0.6820997121220114, 0.5875898600827402]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Dense': 0, 'Dense_1': 1, 'Dropout': 0.26007485780590145, 'Dropout_1': 0.6257491042113806, 'batch_size': 2, 'optimizer': 0}\n"
     ]
    }
   ],
   "source": [
    "def create_model(X_train, X_test, y_train, y_test):\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
    "    model.add(Dense({{choice([32,64, 128 ,256, 512, 1024])}}, activation='relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([32,64, 128 ,256, 512, 1024])}}, activation='relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer={{choice(['rmsprop', 'adam', 'sgd'])}},\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size={{choice([32, 64, 128])}}, validation_data=(np.array(X_test), np.array(y_test)), callbacks=[EarlyStopping(monitor='val_loss', mode='auto')])\n",
    "    score, acc = model.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=10,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='fully_connected_binary')\n",
    "X_train, X_test, y_train, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(np.array(X_test), np.array(y_test)))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save best model\n",
    "def save_model(model,model_name, weights_name):\n",
    "    model_json = model.to_json()\n",
    "    with open(model_name, \"w\") as f:\n",
    "        f.write(model_json)\n",
    "    model.save_weights(weights_name)\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "save_model(best_model, 'models/binary_model.json', 'models/binary_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44235024]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.predict(np.array(X_train[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
