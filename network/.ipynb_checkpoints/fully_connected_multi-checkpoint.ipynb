{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peternagy/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/peternagy/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import RMSprop\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pl\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data loading and preprocessing for Hyperas\n",
    "'''\n",
    "def data():\n",
    "    #I have handpicked columns based on intuition after a handful of tries with all features or Dimensionality Reduction produced poor results\n",
    "    cols = [' title_sentiment_polarity', ' n_tokens_title', ' data_channel_is_lifestyle', ' data_channel_is_entertainment', ' data_channel_is_bus', ' data_channel_is_socmed', ' data_channel_is_tech', ' data_channel_is_world', ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday', ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday', ' weekday_is_sunday', ' is_weekend', ' global_sentiment_polarity', ' shares']\n",
    "    raw_data = pd.read_csv('data/OnlineNewsPopularity.csv', usecols=cols)\n",
    "    \n",
    "    #Dividing records into one of 4 classes. I tried to divide them in a way that the data stays balanced\n",
    "    def categorizeShares(shares):\n",
    "        if shares <= 945:   #FEW\n",
    "            return 0        \n",
    "        if shares <= 1400:  #MODEST\n",
    "            return 1        \n",
    "        if shares <= 2700:  #LOT\n",
    "            return 2        \n",
    "        return 3            #POPULAR\n",
    "    \n",
    "    \n",
    "    #The numerical features are logarithmic scaled (based on the dataset authors) making my job relatively easy on preprocessing.\n",
    "    \n",
    "    multi_class = raw_data.copy(deep=True)\n",
    "    multi_class['label'] = multi_class[' shares'].apply(categorizeShares)\n",
    "    multi_class['label'].value_counts()\n",
    "    multi_class.drop([' shares'], axis=1, inplace=True)\n",
    "\n",
    "    #One-hot encoding labels\n",
    "    y = multi_class['label']\n",
    "    y = to_categorical(y, num_classes=4)\n",
    "    multi_class.drop(['label'], axis=1, inplace=True)\n",
    "    \n",
    "    #Fill possible corrupted data & shuffle rows\n",
    "    multi_class.fillna(\"\", inplace=True)\n",
    "    multi_class = multi_class.reindex()\n",
    "    \n",
    "    #Picking \"best\" features based on feature importance\n",
    "    def constructBestFeatures(df):\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        if 'url' in multi_class.keys():\n",
    "            multi_class.drop(['url'], axis=1, inplace=True)\n",
    "\n",
    "        classifier = DecisionTreeClassifier()\n",
    "        classifier.fit(multi_class, y)\n",
    "\n",
    "\n",
    "        importances = []\n",
    "        for name, importance in zip(multi_class.columns, classifier.feature_importances_):\n",
    "            importances.append((name, importance))\n",
    "\n",
    "        num_cols = len(multi_class.columns)\n",
    "        #Sorting features by importance\n",
    "        importances_sorted = sorted(importances, key=lambda x: x[1])\n",
    "        importances_sorted\n",
    "        cols = []\n",
    "        \n",
    "        '''\n",
    "        Picking the top 14 features: The number 14 came by fine-tuning, \n",
    "        the top 14 features of the handpicked subset of features seems to be the strongest combination\n",
    "        '''\n",
    "        for imp_tupl in importances_sorted[-14:]:\n",
    "            cols.append(imp_tupl[0])\n",
    "        return cols\n",
    "\n",
    "    cols_m = constructBestFeatures(multi_class)\n",
    "    best_features_m = multi_class[cols_m]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(best_features_m, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For multi-class classification I came up with a fully connected Neural Network.\n",
    "For hyper parameter optimization I am using Hyperas, hence the multiple parameters below.\n",
    "The dataset is relatively small, so I had to be careful choosing validation set in order to have enough training data.\n",
    "20% seems ok.\n",
    "'''\n",
    "def create_model(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape[1])\n",
    "    model = Sequential()\n",
    "    model.add(Dense({{choice([16, 32 ,64, 128, 256, 512])}}, activation='relu', input_dim=X_train.shape[1]))\n",
    "\n",
    "\n",
    "    model.add(Dense({{choice([16, 32 ,64])}}, activation='relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([16, 32 ,64])}}, activation='relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([16, 32 ,64])}}, activation='relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    #For multi-class classification we are using softmax as activation and categorical-crossentropy as loss function\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(optimizer={{choice(['rmsprop', 'adam'])}},\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    #According to my tests (due to the small dataset) 10 epoch is enough most of the time for the loss function to converge.\n",
    "    model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size={{choice([16, 32, 64, 128])}}, validation_data=(np.array(X_test), np.array(y_test)),callbacks=[EarlyStopping(monitor='val_loss', mode='auto')])\n",
    "    score, acc = model.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Flatten, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import RMSprop\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as pl\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.tree import DecisionTreeClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [16, 32 ,64, 128, 256, 512]),\n",
      "        'Dense_1': hp.choice('Dense_1', [16, 32 ,64]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dense_2': hp.choice('Dense_2', [16, 32 ,64]),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'Dense_3': hp.choice('Dense_3', [16, 32 ,64]),\n",
      "        'Dropout_2': hp.uniform('Dropout_2', 0, 1),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam']),\n",
      "        'batch_size': hp.choice('batch_size', [16, 32, 64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: #I have handpicked columns based on intuition after a handful of tries with all features or Dimensionality Reduction produced poor results\n",
      "   3: cols = [' title_sentiment_polarity', ' n_tokens_title', ' data_channel_is_lifestyle', ' data_channel_is_entertainment', ' data_channel_is_bus', ' data_channel_is_socmed', ' data_channel_is_tech', ' data_channel_is_world', ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday', ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday', ' weekday_is_sunday', ' is_weekend', ' global_sentiment_polarity', ' shares']\n",
      "   4: raw_data = pd.read_csv('data/OnlineNewsPopularity.csv', usecols=cols)\n",
      "   5: \n",
      "   6: #Dividing records into one of 4 classes. I tried to divide them in a way that the data stays balanced\n",
      "   7: def categorizeShares(shares):\n",
      "   8:     if shares <= 945:   #FEW\n",
      "   9:         return 0        \n",
      "  10:     if shares <= 1400:  #MODEST\n",
      "  11:         return 1        \n",
      "  12:     if shares <= 2700:  #LOT\n",
      "  13:         return 2        \n",
      "  14:     return 3            #POPULAR\n",
      "  15: \n",
      "  16: multi_class = raw_data.copy(deep=True)\n",
      "  17: multi_class['label'] = multi_class[' shares'].apply(categorizeShares)\n",
      "  18: multi_class['label'].value_counts()\n",
      "  19: multi_class.drop([' shares'], axis=1, inplace=True)\n",
      "  20: \n",
      "  21: #One-hot encoding labels\n",
      "  22: y = multi_class['label']\n",
      "  23: y = to_categorical(y, num_classes=4)\n",
      "  24: multi_class.drop(['label'], axis=1, inplace=True)\n",
      "  25: \n",
      "  26: #Fill possible corrupted data & shuffle rows\n",
      "  27: multi_class.fillna(\"\", inplace=True)\n",
      "  28: multi_class = multi_class.reindex()\n",
      "  29: \n",
      "  30: #Picking \"best\" features based on feature importance\n",
      "  31: def constructBestFeatures(df):\n",
      "  32:     from sklearn.tree import DecisionTreeClassifier\n",
      "  33:     if 'url' in multi_class.keys():\n",
      "  34:         multi_class.drop(['url'], axis=1, inplace=True)\n",
      "  35: \n",
      "  36:     classifier = DecisionTreeClassifier()\n",
      "  37:     classifier.fit(multi_class, y)\n",
      "  38: \n",
      "  39: \n",
      "  40:     importances = []\n",
      "  41:     for name, importance in zip(multi_class.columns, classifier.feature_importances_):\n",
      "  42:         importances.append((name, importance))\n",
      "  43: \n",
      "  44:     num_cols = len(multi_class.columns)\n",
      "  45:     #Sorting features by importance\n",
      "  46:     importances_sorted = sorted(importances, key=lambda x: x[1])\n",
      "  47:     importances_sorted\n",
      "  48:     cols = []\n",
      "  49:     \n",
      "  50:     '''\n",
      "  51:     Picking the top 14 features: The number 14 came by fine-tuning, \n",
      "  52:     the top 14 features of the handpicked subset of features seems to be the strongest combination\n",
      "  53:     '''\n",
      "  54:     for imp_tupl in importances_sorted[-14:]:\n",
      "  55:         cols.append(imp_tupl[0])\n",
      "  56:     return cols\n",
      "  57: \n",
      "  58: cols_m = constructBestFeatures(multi_class)\n",
      "  59: best_features_m = multi_class[cols_m]\n",
      "  60: \n",
      "  61: X_train, X_test, y_train, y_test = train_test_split(best_features_m, y, test_size=0.2, random_state=42)\n",
      "  62: \n",
      "  63: \n",
      "  64: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \n",
      "   4:     print(X_train.shape, y_train.shape, X_test.shape, y_test.shape[1])\n",
      "   5:     model = Sequential()\n",
      "   6:     model.add(Dense(space['Dense'], activation='relu', input_dim=X_train.shape[1]))\n",
      "   7: \n",
      "   8: \n",
      "   9:     model.add(Dense(space['Dense_1'], activation='relu'))\n",
      "  10:     model.add(Dropout(space['Dropout']))\n",
      "  11:     model.add(Dense(space['Dense_2'], activation='relu'))\n",
      "  12:     model.add(Dropout(space['Dropout_1']))\n",
      "  13:     model.add(Dense(space['Dense_3'], activation='relu'))\n",
      "  14:     model.add(Dropout(space['Dropout_2']))\n",
      "  15:     \n",
      "  16:     #For multi-class classification we are using softmax as activation and categorical-crossentropy as loss function\n",
      "  17:     model.add(Dense(4, activation='softmax'))\n",
      "  18:     model.compile(optimizer=space['optimizer'],\n",
      "  19:                   loss='categorical_crossentropy',\n",
      "  20:                   metrics=['accuracy'])\n",
      "  21: \n",
      "  22:     #According to my tests (due to the small dataset) 10 epoch is enough most of the time for the loss function to converge.\n",
      "  23:     model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=space['batch_size'], validation_data=(np.array(X_test), np.array(y_test)),callbacks=[EarlyStopping(monitor='val_loss', mode='auto')])\n",
      "  24:     score, acc = model.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
      "  25:     print('Test accuracy:', acc)\n",
      "  26:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  27: \n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 2s - loss: 1.3895 - acc: 0.2526 - val_loss: 1.3860 - val_acc: 0.2588\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.3865 - acc: 0.2548 - val_loss: 1.3857 - val_acc: 0.2588\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 2s - loss: 1.3864 - acc: 0.2536 - val_loss: 1.3861 - val_acc: 0.2530\n",
      "Test accuracy: 0.252995333574296\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.4399 - acc: 0.2523 - val_loss: 1.3856 - val_acc: 0.2588\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3867 - acc: 0.2562 - val_loss: 1.3858 - val_acc: 0.2588\n",
      "Test accuracy: 0.25879682178214064\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.4231 - acc: 0.2499 - val_loss: 1.3857 - val_acc: 0.2588\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.3862 - acc: 0.2547 - val_loss: 1.3857 - val_acc: 0.2588\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.3862 - acc: 0.2553 - val_loss: 1.3858 - val_acc: 0.2588\n",
      "Test accuracy: 0.25879682178214064\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 2s - loss: 1.3885 - acc: 0.2545 - val_loss: 1.3857 - val_acc: 0.2588\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.3863 - acc: 0.2555 - val_loss: 1.3859 - val_acc: 0.2588\n",
      "Test accuracy: 0.25879682178214064\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 3s - loss: 1.3923 - acc: 0.2697 - val_loss: 1.3711 - val_acc: 0.3106\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 3s - loss: 1.3723 - acc: 0.2905 - val_loss: 1.3681 - val_acc: 0.3140\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31715/31715 [==============================] - 3s - loss: 1.3659 - acc: 0.3044 - val_loss: 1.3618 - val_acc: 0.3216\n",
      "Epoch 4/10\n",
      "31715/31715 [==============================] - 3s - loss: 1.3592 - acc: 0.3155 - val_loss: 1.3593 - val_acc: 0.3225\n",
      "Epoch 5/10\n",
      "31715/31715 [==============================] - 3s - loss: 1.3594 - acc: 0.3147 - val_loss: 1.3653 - val_acc: 0.2944\n",
      "Test accuracy: 0.2943624669387852\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 0s - loss: 2.0515 - acc: 0.2517 - val_loss: 1.3863 - val_acc: 0.2530\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3877 - acc: 0.2535 - val_loss: 1.3859 - val_acc: 0.2588\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3865 - acc: 0.2569 - val_loss: 1.3859 - val_acc: 0.2588\n",
      "Epoch 4/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3864 - acc: 0.2563 - val_loss: 1.3858 - val_acc: 0.2588\n",
      "Epoch 5/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3863 - acc: 0.2585 - val_loss: 1.3857 - val_acc: 0.2588\n",
      "Epoch 6/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3862 - acc: 0.2555 - val_loss: 1.3859 - val_acc: 0.2588\n",
      "Test accuracy: 0.25879682178214064\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.4094 - acc: 0.2497 - val_loss: 1.3859 - val_acc: 0.2395\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3827 - acc: 0.2686 - val_loss: 1.3703 - val_acc: 0.3427\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3711 - acc: 0.3026 - val_loss: 1.3557 - val_acc: 0.3367\n",
      "Epoch 4/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3555 - acc: 0.3209 - val_loss: 1.3466 - val_acc: 0.3383\n",
      "Epoch 5/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3494 - acc: 0.3289 - val_loss: 1.3446 - val_acc: 0.3408\n",
      "Epoch 6/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3480 - acc: 0.3309 - val_loss: 1.3432 - val_acc: 0.3434\n",
      "Epoch 7/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3452 - acc: 0.3329 - val_loss: 1.3438 - val_acc: 0.3458\n",
      "Test accuracy: 0.34581914492987914\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 2s - loss: 1.3826 - acc: 0.2664 - val_loss: 1.3674 - val_acc: 0.3171\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.3738 - acc: 0.2855 - val_loss: 1.3614 - val_acc: 0.3105\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.3687 - acc: 0.2972 - val_loss: 1.3715 - val_acc: 0.2795\n",
      "Test accuracy: 0.2794803884305574\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.5064 - acc: 0.2513 - val_loss: 1.3858 - val_acc: 0.2530\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3866 - acc: 0.2513 - val_loss: 1.3857 - val_acc: 0.2588\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3859 - acc: 0.2532 - val_loss: 1.3854 - val_acc: 0.2588\n",
      "Epoch 4/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3853 - acc: 0.2573 - val_loss: 1.3837 - val_acc: 0.2854\n",
      "Epoch 5/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3851 - acc: 0.2599 - val_loss: 1.3837 - val_acc: 0.2797\n",
      "Test accuracy: 0.27973262705392776\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 3s - loss: 1.3812 - acc: 0.2845 - val_loss: 1.3425 - val_acc: 0.3395\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 3s - loss: 1.3480 - acc: 0.3287 - val_loss: 1.3416 - val_acc: 0.3365\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 3s - loss: 1.3449 - acc: 0.3297 - val_loss: 1.3444 - val_acc: 0.3357\n",
      "Test accuracy: 0.3357296002205841\n",
      "Evalutation of best performing model:\n",
      "5856/7929 [=====================>........] - ETA: 0s[1.3437167493733833, 0.34733257663627337]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Dense': 0, 'Dense_1': 1, 'Dense_2': 1, 'Dense_3': 0, 'Dropout': 0.5422412690636627, 'Dropout_1': 0.33351393608141355, 'Dropout_2': 0.10530729142803252, 'batch_size': 3, 'optimizer': 1}\n"
     ]
    }
   ],
   "source": [
    "#Running hyper-param optimalization and choosing the best model\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=10,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='fully_connected_multi')\n",
    "X_train, X_test, y_train, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(np.array(X_test), np.array(y_test)))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save best model\n",
    "def save_model(model,model_name, weights_name):\n",
    "    model_json = model.to_json()\n",
    "    with open(model_name, \"w\") as f:\n",
    "        f.write(model_json)\n",
    "    model.save_weights(weights_name)\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "save_model(best_model, 'models/multi_model.json', 'models/multi_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19837</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.047722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_channel_is_socmed   data_channel_is_tech   data_channel_is_bus  \\\n",
       "19837                      0.0                    0.0                   1.0   \n",
       "\n",
       "        data_channel_is_world   data_channel_is_entertainment   is_weekend  \\\n",
       "19837                     0.0                             0.0          0.0   \n",
       "\n",
       "        weekday_is_friday   weekday_is_monday   weekday_is_tuesday  \\\n",
       "19837                 0.0                 0.0                  1.0   \n",
       "\n",
       "        weekday_is_wednesday   weekday_is_thursday   title_sentiment_polarity  \\\n",
       "19837                    0.0                   0.0                   0.333333   \n",
       "\n",
       "        n_tokens_title   global_sentiment_polarity  \n",
       "19837             15.0                    0.047722  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(best_model.predict(np.array(X_train[0:1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
