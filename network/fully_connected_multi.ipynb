{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peternagy/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/peternagy/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import RMSprop\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pl\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data loading and preprocessing for Hyperas\n",
    "'''\n",
    "def data():\n",
    "    #I have handpicked columns based on intuition after a handful of tries with all features or Dimensionality Reduction produced poor results\n",
    "    cols = [' title_sentiment_polarity', ' n_tokens_title', ' data_channel_is_lifestyle', ' data_channel_is_entertainment', ' data_channel_is_bus', ' data_channel_is_socmed', ' data_channel_is_tech', ' data_channel_is_world', ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday', ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday', ' weekday_is_sunday', ' is_weekend', ' global_sentiment_polarity', ' shares']\n",
    "    raw_data = pd.read_csv('data/OnlineNewsPopularity.csv', usecols=cols)\n",
    "    \n",
    "    #Standardize n_tokens_title\n",
    "    raw_data[' n_tokens_title'] -= raw_data[' n_tokens_title'].mean(axis=0)\n",
    "    raw_data[' n_tokens_title'] /= raw_data[' n_tokens_title'].std(axis=0)\n",
    "    \n",
    "    #Dividing records into one of 4 classes. I tried to divide them in a way that the data stays balanced\n",
    "    def categorizeShares(shares):\n",
    "        if shares <= 945:   #FEW\n",
    "            return 0        \n",
    "        if shares <= 1400:  #MODEST\n",
    "            return 1        \n",
    "        if shares <= 2700:  #LOT\n",
    "            return 2        \n",
    "        return 3            #POPULAR\n",
    "    \n",
    "    \n",
    "    #The numerical features are logarithmic scaled (based on the dataset authors) making my job relatively easy on preprocessing.\n",
    "    \n",
    "    multi_class = raw_data.copy(deep=True)\n",
    "    multi_class['label'] = multi_class[' shares'].apply(categorizeShares)\n",
    "    multi_class['label'].value_counts()\n",
    "    multi_class.drop([' shares'], axis=1, inplace=True)\n",
    "\n",
    "    #One-hot encoding labels\n",
    "    y = multi_class['label']\n",
    "    y = to_categorical(y, num_classes=4)\n",
    "    multi_class.drop(['label'], axis=1, inplace=True)\n",
    "    \n",
    "    #Fill possible corrupted data & shuffle rows\n",
    "    multi_class.fillna(\"\", inplace=True)\n",
    "    multi_class = multi_class.reindex()\n",
    "    \n",
    "    #Picking \"best\" features based on feature importance\n",
    "    def constructBestFeatures(df):\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        if 'url' in multi_class.keys():\n",
    "            multi_class.drop(['url'], axis=1, inplace=True)\n",
    "\n",
    "        classifier = DecisionTreeClassifier()\n",
    "        classifier.fit(multi_class, y)\n",
    "\n",
    "\n",
    "        importances = []\n",
    "        for name, importance in zip(multi_class.columns, classifier.feature_importances_):\n",
    "            importances.append((name, importance))\n",
    "\n",
    "        num_cols = len(multi_class.columns)\n",
    "        #Sorting features by importance\n",
    "        importances_sorted = sorted(importances, key=lambda x: x[1])\n",
    "        importances_sorted\n",
    "        cols = []\n",
    "        print(importances_sorted)\n",
    "        for imp_tupl in importances_sorted:\n",
    "            cols.append(imp_tupl[0])\n",
    "        return cols\n",
    "\n",
    "    cols_m = constructBestFeatures(multi_class)\n",
    "    best_features_m = multi_class[cols_m]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(best_features_m, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For multi-class classification I came up with a fully connected Neural Network.\n",
    "For hyper parameter optimization I am using Hyperas, hence the multiple parameters below.\n",
    "The dataset is relatively small, so I had to be careful choosing validation set in order to have enough training data.\n",
    "10~20% seems ok.\n",
    "'''\n",
    "def create_model(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape[1])\n",
    "    model = Sequential()\n",
    "    model.add(Dense({{choice([16, 32 ,64, 128, 256, 512])}}, activation='relu', input_dim=X_train.shape[1]))\n",
    "\n",
    "\n",
    "    model.add(Dense({{choice([16, 32 ,64])}}, activation='relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([16, 32 ,64])}}, activation='relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([16, 32 ,64])}}, activation='relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    #For multi-class classification we are using softmax as activation and categorical-crossentropy as loss function\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(optimizer={{choice(['rmsprop', 'adam'])}},\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    #According to my tests (due to the small dataset) 10 epoch is enough most of the time for the loss function to converge.\n",
    "    model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size={{choice([16, 32, 64, 128])}}, validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', mode='auto')])\n",
    "    score, acc = model.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Flatten, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import RMSprop\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as pl\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.tree import DecisionTreeClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [16, 32 ,64, 128, 256, 512]),\n",
      "        'Dense_1': hp.choice('Dense_1', [16, 32 ,64]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dense_2': hp.choice('Dense_2', [16, 32 ,64]),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'Dense_3': hp.choice('Dense_3', [16, 32 ,64]),\n",
      "        'Dropout_2': hp.uniform('Dropout_2', 0, 1),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam']),\n",
      "        'batch_size': hp.choice('batch_size', [16, 32, 64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: #I have handpicked columns based on intuition after a handful of tries with all features or Dimensionality Reduction produced poor results\n",
      "   3: cols = [' title_sentiment_polarity', ' n_tokens_title', ' data_channel_is_lifestyle', ' data_channel_is_entertainment', ' data_channel_is_bus', ' data_channel_is_socmed', ' data_channel_is_tech', ' data_channel_is_world', ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday', ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday', ' weekday_is_sunday', ' is_weekend', ' global_sentiment_polarity', ' shares']\n",
      "   4: raw_data = pd.read_csv('data/OnlineNewsPopularity.csv', usecols=cols)\n",
      "   5: \n",
      "   6: #Standardize n_tokens_title\n",
      "   7: raw_data[' n_tokens_title'] -= raw_data[' n_tokens_title'].mean(axis=0)\n",
      "   8: raw_data[' n_tokens_title'] /= raw_data[' n_tokens_title'].std(axis=0)\n",
      "   9: \n",
      "  10: #Dividing records into one of 4 classes. I tried to divide them in a way that the data stays balanced\n",
      "  11: def categorizeShares(shares):\n",
      "  12:     if shares <= 945:   #FEW\n",
      "  13:         return 0        \n",
      "  14:     if shares <= 1400:  #MODEST\n",
      "  15:         return 1        \n",
      "  16:     if shares <= 2700:  #LOT\n",
      "  17:         return 2        \n",
      "  18:     return 3            #POPULAR\n",
      "  19: \n",
      "  20: \n",
      "  21: #The numerical features are logarithmic scaled (based on the dataset authors) making my job relatively easy on preprocessing.\n",
      "  22: \n",
      "  23: multi_class = raw_data.copy(deep=True)\n",
      "  24: multi_class['label'] = multi_class[' shares'].apply(categorizeShares)\n",
      "  25: multi_class['label'].value_counts()\n",
      "  26: multi_class.drop([' shares'], axis=1, inplace=True)\n",
      "  27: \n",
      "  28: #One-hot encoding labels\n",
      "  29: y = multi_class['label']\n",
      "  30: y = to_categorical(y, num_classes=4)\n",
      "  31: multi_class.drop(['label'], axis=1, inplace=True)\n",
      "  32: \n",
      "  33: #Fill possible corrupted data & shuffle rows\n",
      "  34: multi_class.fillna(\"\", inplace=True)\n",
      "  35: multi_class = multi_class.reindex()\n",
      "  36: \n",
      "  37: #Picking \"best\" features based on feature importance\n",
      "  38: def constructBestFeatures(df):\n",
      "  39:     from sklearn.tree import DecisionTreeClassifier\n",
      "  40:     if 'url' in multi_class.keys():\n",
      "  41:         multi_class.drop(['url'], axis=1, inplace=True)\n",
      "  42: \n",
      "  43:     classifier = DecisionTreeClassifier()\n",
      "  44:     classifier.fit(multi_class, y)\n",
      "  45: \n",
      "  46: \n",
      "  47:     importances = []\n",
      "  48:     for name, importance in zip(multi_class.columns, classifier.feature_importances_):\n",
      "  49:         importances.append((name, importance))\n",
      "  50: \n",
      "  51:     num_cols = len(multi_class.columns)\n",
      "  52:     #Sorting features by importance\n",
      "  53:     importances_sorted = sorted(importances, key=lambda x: x[1])\n",
      "  54:     importances_sorted\n",
      "  55:     cols = []\n",
      "  56:     print(importances_sorted)\n",
      "  57:     for imp_tupl in importances_sorted:\n",
      "  58:         cols.append(imp_tupl[0])\n",
      "  59:     return cols\n",
      "  60: \n",
      "  61: cols_m = constructBestFeatures(multi_class)\n",
      "  62: best_features_m = multi_class[cols_m]\n",
      "  63: \n",
      "  64: X_train, X_test, y_train, y_test = train_test_split(best_features_m, y, test_size=0.2, random_state=42)\n",
      "  65: \n",
      "  66: \n",
      "  67: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \n",
      "   4:     print(X_train.shape, y_train.shape, X_test.shape, y_test.shape[1])\n",
      "   5:     model = Sequential()\n",
      "   6:     model.add(Dense(space['Dense'], activation='relu', input_dim=X_train.shape[1]))\n",
      "   7: \n",
      "   8: \n",
      "   9:     model.add(Dense(space['Dense_1'], activation='relu'))\n",
      "  10:     model.add(Dropout(space['Dropout']))\n",
      "  11:     model.add(Dense(space['Dense_2'], activation='relu'))\n",
      "  12:     model.add(Dropout(space['Dropout_1']))\n",
      "  13:     model.add(Dense(space['Dense_3'], activation='relu'))\n",
      "  14:     model.add(Dropout(space['Dropout_2']))\n",
      "  15:     \n",
      "  16:     #For multi-class classification we are using softmax as activation and categorical-crossentropy as loss function\n",
      "  17:     model.add(Dense(4, activation='softmax'))\n",
      "  18:     model.compile(optimizer=space['optimizer'],\n",
      "  19:                   loss='categorical_crossentropy',\n",
      "  20:                   metrics=['accuracy'])\n",
      "  21: \n",
      "  22:     #According to my tests (due to the small dataset) 10 epoch is enough most of the time for the loss function to converge.\n",
      "  23:     model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=space['batch_size'], validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', mode='auto')])\n",
      "  24:     score, acc = model.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
      "  25:     print('Test accuracy:', acc)\n",
      "  26:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  27: \n",
      "[(' weekday_is_saturday', 0.0014755124879845783), (' weekday_is_sunday', 0.0016374576097365875), (' data_channel_is_lifestyle', 0.002786559061710452), (' data_channel_is_socmed', 0.003166615423378054), (' data_channel_is_tech', 0.00504351553152572), (' data_channel_is_bus', 0.005714265406976308), (' data_channel_is_world', 0.009216984816256315), (' data_channel_is_entertainment', 0.009328545082058447), (' is_weekend', 0.00950311122760864), (' weekday_is_friday', 0.01196579726167872), (' weekday_is_monday', 0.019310699468398307), (' weekday_is_tuesday', 0.0220383433785308), (' weekday_is_thursday', 0.023484979539645647), (' weekday_is_wednesday', 0.0247662908032132), (' title_sentiment_polarity', 0.14408007585642965), (' n_tokens_title', 0.15527979050896284), (' global_sentiment_polarity', 0.5512014565359059)]\n",
      "(31715, 17) (31715, 4) (7929, 17) 4\n",
      "Train on 28543 samples, validate on 3172 samples\n",
      "Epoch 1/10\n",
      "28543/28543 [==============================] - 3s - loss: 1.3841 - acc: 0.2694 - val_loss: 1.3688 - val_acc: 0.3181\n",
      "Epoch 2/10\n",
      "28543/28543 [==============================] - 3s - loss: 1.3683 - acc: 0.3079 - val_loss: 1.3567 - val_acc: 0.3200\n",
      "Epoch 3/10\n",
      "28543/28543 [==============================] - 2s - loss: 1.3643 - acc: 0.3069 - val_loss: 1.3609 - val_acc: 0.3020\n",
      "Test accuracy: 0.3029385799153754\n",
      "(31715, 17) (31715, 4) (7929, 17) 4\n",
      "Train on 28543 samples, validate on 3172 samples\n",
      "Epoch 1/10\n",
      "28543/28543 [==============================] - 2s - loss: 1.4129 - acc: 0.2529 - val_loss: 1.3862 - val_acc: 0.2487\n",
      "Epoch 2/10\n",
      "28543/28543 [==============================] - 1s - loss: 1.3866 - acc: 0.2549 - val_loss: 1.3861 - val_acc: 0.2487\n",
      "Epoch 3/10\n",
      "28543/28543 [==============================] - 1s - loss: 1.3866 - acc: 0.2542 - val_loss: 1.3864 - val_acc: 0.2487\n",
      "Test accuracy: 0.25879682178214064\n",
      "(31715, 17) (31715, 4) (7929, 17) 4\n",
      "Train on 28543 samples, validate on 3172 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28543/28543 [==============================] - 2s - loss: 1.3939 - acc: 0.2552 - val_loss: 1.3862 - val_acc: 0.2487\n",
      "Epoch 2/10\n",
      "28543/28543 [==============================] - 1s - loss: 1.3866 - acc: 0.2569 - val_loss: 1.3864 - val_acc: 0.2487\n",
      "Test accuracy: 0.25879682178214064\n",
      "(31715, 17) (31715, 4) (7929, 17) 4\n",
      "Train on 28543 samples, validate on 3172 samples\n",
      "Epoch 1/10\n",
      "14304/28543 [==============>...............] - ETA: 2s - loss: 1.3912 - acc: 0.2597"
     ]
    }
   ],
   "source": [
    "#Running hyper-param optimalization and choosing the best model\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=10,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='fully_connected_multi')\n",
    "X_train, X_test, y_train, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(np.array(X_test), np.array(y_test)))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save best model\n",
    "def save_model(model,model_name, weights_name):\n",
    "    model_json = model.to_json()\n",
    "    with open(model_name, \"w\") as f:\n",
    "        f.write(model_json)\n",
    "    model.save_weights(weights_name)\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "save_model(best_model, 'models/multi_model.json', 'models/multi_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19837</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.176524</td>\n",
       "      <td>0.047722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        weekday_is_saturday   weekday_is_sunday   data_channel_is_lifestyle  \\\n",
       "19837                   0.0                 0.0                         0.0   \n",
       "\n",
       "        data_channel_is_socmed   data_channel_is_tech   data_channel_is_bus  \\\n",
       "19837                      0.0                    0.0                   1.0   \n",
       "\n",
       "        data_channel_is_world   data_channel_is_entertainment   is_weekend  \\\n",
       "19837                     0.0                             0.0          0.0   \n",
       "\n",
       "        weekday_is_friday   weekday_is_monday   weekday_is_tuesday  \\\n",
       "19837                 0.0                 0.0                  1.0   \n",
       "\n",
       "        weekday_is_thursday   weekday_is_wednesday   title_sentiment_polarity  \\\n",
       "19837                   0.0                    0.0                   0.333333   \n",
       "\n",
       "        n_tokens_title   global_sentiment_polarity  \n",
       "19837         2.176524                    0.047722  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(best_model.predict(np.array(X_train[0:1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
