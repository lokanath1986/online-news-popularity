{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peternagy/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/peternagy/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import RMSprop\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pl\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data loading and preprocessing for Hyperas\n",
    "'''\n",
    "def data():\n",
    "    #I have handpicked columns based on intuition after a handful of tries with all features or Dimensionality Reduction produced poor results\n",
    "    cols = [' title_sentiment_polarity', ' n_tokens_title', ' data_channel_is_lifestyle', ' data_channel_is_entertainment', ' data_channel_is_bus', ' data_channel_is_socmed', ' data_channel_is_tech', ' data_channel_is_world', ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday', ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday', ' weekday_is_sunday', ' is_weekend', ' global_sentiment_polarity', ' shares']\n",
    "    raw_data = pd.read_csv('data/OnlineNewsPopularity.csv', usecols=cols)\n",
    "    \n",
    "    #Dividing records into one of 4 classes. I tried to divide them in a way that the data stays balanced\n",
    "    def categorizeShares(shares):\n",
    "        if shares <= 945:   #FEW\n",
    "            return 0        \n",
    "        if shares <= 1400:  #MODEST\n",
    "            return 1        \n",
    "        if shares <= 2700:  #LOT\n",
    "            return 2        \n",
    "        return 3            #POPULAR\n",
    "    \n",
    "    \n",
    "    #The numerical features are logarithmic scaled (based on the dataset authors) making my job relatively easy on preprocessing.\n",
    "    \n",
    "    multi_class = raw_data.copy(deep=True)\n",
    "    multi_class['label'] = multi_class[' shares'].apply(categorizeShares)\n",
    "    multi_class['label'].value_counts()\n",
    "    multi_class.drop([' shares'], axis=1, inplace=True)\n",
    "\n",
    "    #One-hot encoding labels\n",
    "    y = multi_class['label']\n",
    "    y = to_categorical(y, num_classes=4)\n",
    "    multi_class.drop(['label'], axis=1, inplace=True)\n",
    "    \n",
    "    #Fill possible corrupted data & shuffle rows\n",
    "    multi_class.fillna(\"\", inplace=True)\n",
    "    multi_class = multi_class.reindex()\n",
    "    \n",
    "    #Picking \"best\" features based on feature importance\n",
    "    def constructBestFeatures(df):\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        if 'url' in multi_class.keys():\n",
    "            multi_class.drop(['url'], axis=1, inplace=True)\n",
    "\n",
    "        classifier = DecisionTreeClassifier()\n",
    "        classifier.fit(multi_class, y)\n",
    "\n",
    "\n",
    "        importances = []\n",
    "        for name, importance in zip(multi_class.columns, classifier.feature_importances_):\n",
    "            importances.append((name, importance))\n",
    "\n",
    "        num_cols = len(multi_class.columns)\n",
    "        #Sorting features by importance\n",
    "        importances_sorted = sorted(importances, key=lambda x: x[1])\n",
    "        importances_sorted\n",
    "        cols = []\n",
    "        \n",
    "        '''\n",
    "        Picking the top 14 features: The number 14 came by fine-tuning, \n",
    "        the top 14 features of the handpicked subset of features seems to be the strongest combination\n",
    "        '''\n",
    "        for imp_tupl in importances_sorted[-14:]:\n",
    "            cols.append(imp_tupl[0])\n",
    "        return cols\n",
    "\n",
    "    cols_m = constructBestFeatures(multi_class)\n",
    "    best_features_m = multi_class[cols_m]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(best_features_m, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For multi-class classification I came up with a fully connected Neural Network.\n",
    "For hyper parameter optimization I am using Hyperas, hence the multiple parameters below.\n",
    "The dataset is relatively small, so I had to be careful choosing validation set in order to have enough training data.\n",
    "20% seems ok.\n",
    "'''\n",
    "def create_model(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape[1])\n",
    "    model = Sequential()\n",
    "    model.add(Dense({{choice([16, 32 ,64, 128, 256, 512])}}, activation='relu', input_dim=X_train.shape[1]))\n",
    "\n",
    "\n",
    "    model.add(Dense({{choice([16, 32 ,64])}}, activation='relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([16, 32 ,64])}}, activation='relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([16, 32 ,64])}}, activation='relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    #For multi-class classification we are using softmax as activation and categorical-crossentropy as loss function\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(optimizer={{choice(['rmsprop', 'adam'])}},\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    #According to my tests (due to the small dataset) 10 epoch is enough most of the time for the loss function to converge.\n",
    "    model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size={{choice([16, 32, 64, 128])}}, validation_data=(np.array(X_test), np.array(y_test)),callbacks=[EarlyStopping(monitor='val_loss', mode='auto')])\n",
    "    score, acc = model.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Flatten, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import RMSprop\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as pl\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.tree import DecisionTreeClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [16, 32 ,64, 128, 256, 512]),\n",
      "        'Dense_1': hp.choice('Dense_1', [16, 32 ,64]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dense_2': hp.choice('Dense_2', [16, 32 ,64]),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'Dense_3': hp.choice('Dense_3', [16, 32 ,64]),\n",
      "        'Dropout_2': hp.uniform('Dropout_2', 0, 1),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam']),\n",
      "        'batch_size': hp.choice('batch_size', [16, 32, 64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: #I have handpicked columns based on intuition after a handful of tries with all features or Dimensionality Reduction produced poor results\n",
      "   3: cols = [' title_sentiment_polarity', ' n_tokens_title', ' data_channel_is_lifestyle', ' data_channel_is_entertainment', ' data_channel_is_bus', ' data_channel_is_socmed', ' data_channel_is_tech', ' data_channel_is_world', ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday', ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday', ' weekday_is_sunday', ' is_weekend', ' global_sentiment_polarity', ' shares']\n",
      "   4: raw_data = pd.read_csv('data/OnlineNewsPopularity.csv', usecols=cols)\n",
      "   5: \n",
      "   6: #Dividing records into one of 4 classes. I tried to divide them in a way that the data stays balanced\n",
      "   7: def categorizeShares(shares):\n",
      "   8:     if shares <= 945:   #FEW\n",
      "   9:         return 0        \n",
      "  10:     if shares <= 1400:  #MODEST\n",
      "  11:         return 1        \n",
      "  12:     if shares <= 2700:  #LOT\n",
      "  13:         return 2        \n",
      "  14:     return 3            #POPULAR\n",
      "  15: \n",
      "  16: \n",
      "  17: #The numerical features are logarithmic scaled (based on the dataset authors) making my job relatively easy on preprocessing.\n",
      "  18: \n",
      "  19: multi_class = raw_data.copy(deep=True)\n",
      "  20: multi_class['label'] = multi_class[' shares'].apply(categorizeShares)\n",
      "  21: multi_class['label'].value_counts()\n",
      "  22: multi_class.drop([' shares'], axis=1, inplace=True)\n",
      "  23: \n",
      "  24: #One-hot encoding labels\n",
      "  25: y = multi_class['label']\n",
      "  26: y = to_categorical(y, num_classes=4)\n",
      "  27: multi_class.drop(['label'], axis=1, inplace=True)\n",
      "  28: \n",
      "  29: #Fill possible corrupted data & shuffle rows\n",
      "  30: multi_class.fillna(\"\", inplace=True)\n",
      "  31: multi_class = multi_class.reindex()\n",
      "  32: \n",
      "  33: #Picking \"best\" features based on feature importance\n",
      "  34: def constructBestFeatures(df):\n",
      "  35:     from sklearn.tree import DecisionTreeClassifier\n",
      "  36:     if 'url' in multi_class.keys():\n",
      "  37:         multi_class.drop(['url'], axis=1, inplace=True)\n",
      "  38: \n",
      "  39:     classifier = DecisionTreeClassifier()\n",
      "  40:     classifier.fit(multi_class, y)\n",
      "  41: \n",
      "  42: \n",
      "  43:     importances = []\n",
      "  44:     for name, importance in zip(multi_class.columns, classifier.feature_importances_):\n",
      "  45:         importances.append((name, importance))\n",
      "  46: \n",
      "  47:     num_cols = len(multi_class.columns)\n",
      "  48:     #Sorting features by importance\n",
      "  49:     importances_sorted = sorted(importances, key=lambda x: x[1])\n",
      "  50:     importances_sorted\n",
      "  51:     cols = []\n",
      "  52:     \n",
      "  53:     '''\n",
      "  54:     Picking the top 14 features: The number 14 came by fine-tuning, \n",
      "  55:     the top 14 features of the handpicked subset of features seems to be the strongest combination\n",
      "  56:     '''\n",
      "  57:     for imp_tupl in importances_sorted[-14:]:\n",
      "  58:         cols.append(imp_tupl[0])\n",
      "  59:     return cols\n",
      "  60: \n",
      "  61: cols_m = constructBestFeatures(multi_class)\n",
      "  62: best_features_m = multi_class[cols_m]\n",
      "  63: \n",
      "  64: X_train, X_test, y_train, y_test = train_test_split(best_features_m, y, test_size=0.2, random_state=42)\n",
      "  65: \n",
      "  66: \n",
      "  67: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \n",
      "   4:     print(X_train.shape, y_train.shape, X_test.shape, y_test.shape[1])\n",
      "   5:     model = Sequential()\n",
      "   6:     model.add(Dense(space['Dense'], activation='relu', input_dim=X_train.shape[1]))\n",
      "   7: \n",
      "   8: \n",
      "   9:     model.add(Dense(space['Dense_1'], activation='relu'))\n",
      "  10:     model.add(Dropout(space['Dropout']))\n",
      "  11:     model.add(Dense(space['Dense_2'], activation='relu'))\n",
      "  12:     model.add(Dropout(space['Dropout_1']))\n",
      "  13:     model.add(Dense(space['Dense_3'], activation='relu'))\n",
      "  14:     model.add(Dropout(space['Dropout_2']))\n",
      "  15:     \n",
      "  16:     #For multi-class classification we are using softmax as activation and categorical-crossentropy as loss function\n",
      "  17:     model.add(Dense(4, activation='softmax'))\n",
      "  18:     model.compile(optimizer=space['optimizer'],\n",
      "  19:                   loss='categorical_crossentropy',\n",
      "  20:                   metrics=['accuracy'])\n",
      "  21: \n",
      "  22:     #According to my tests (due to the small dataset) 10 epoch is enough most of the time for the loss function to converge.\n",
      "  23:     model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=space['batch_size'], validation_data=(np.array(X_test), np.array(y_test)),callbacks=[EarlyStopping(monitor='val_loss', mode='auto')])\n",
      "  24:     score, acc = model.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
      "  25:     print('Test accuracy:', acc)\n",
      "  26:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  27: \n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 2s - loss: 1.3896 - acc: 0.2524 - val_loss: 1.3856 - val_acc: 0.2588\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.3870 - acc: 0.2564 - val_loss: 1.3857 - val_acc: 0.2614\n",
      "Test accuracy: 0.2614453272683306\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.4310 - acc: 0.2529 - val_loss: 1.3858 - val_acc: 0.2588\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3864 - acc: 0.2534 - val_loss: 1.3858 - val_acc: 0.2588\n",
      "Test accuracy: 0.25879682178214064\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.4206 - acc: 0.2518 - val_loss: 1.3859 - val_acc: 0.2588\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.3862 - acc: 0.2562 - val_loss: 1.3858 - val_acc: 0.2588\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.3862 - acc: 0.2562 - val_loss: 1.3858 - val_acc: 0.2588\n",
      "Test accuracy: 0.25879682178214064\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 2s - loss: 1.3902 - acc: 0.2549 - val_loss: 1.3859 - val_acc: 0.2588\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.3863 - acc: 0.2531 - val_loss: 1.3857 - val_acc: 0.2588\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.3862 - acc: 0.2545 - val_loss: 1.3860 - val_acc: 0.2588\n",
      "Test accuracy: 0.25879682178214064\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 3s - loss: 1.3841 - acc: 0.2757 - val_loss: 1.3602 - val_acc: 0.3288\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31715/31715 [==============================] - 3s - loss: 1.3646 - acc: 0.3057 - val_loss: 1.3541 - val_acc: 0.3264\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 3s - loss: 1.3572 - acc: 0.3205 - val_loss: 1.3545 - val_acc: 0.3171\n",
      "Test accuracy: 0.31706394248207787\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.8038 - acc: 0.2466 - val_loss: 1.3861 - val_acc: 0.2551\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3877 - acc: 0.2527 - val_loss: 1.3859 - val_acc: 0.2588\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3864 - acc: 0.2564 - val_loss: 1.3858 - val_acc: 0.2588\n",
      "Epoch 4/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3863 - acc: 0.2563 - val_loss: 1.3858 - val_acc: 0.2588\n",
      "Epoch 5/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3867 - acc: 0.2566 - val_loss: 1.3858 - val_acc: 0.2588\n",
      "Epoch 6/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3863 - acc: 0.2562 - val_loss: 1.3858 - val_acc: 0.2588\n",
      "Epoch 7/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3862 - acc: 0.2565 - val_loss: 1.3859 - val_acc: 0.2588\n",
      "Test accuracy: 0.25879682178214064\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.4095 - acc: 0.2563 - val_loss: 1.3835 - val_acc: 0.27830.25\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3813 - acc: 0.2744 - val_loss: 1.3705 - val_acc: 0.3304\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3681 - acc: 0.3015 - val_loss: 1.3474 - val_acc: 0.3388\n",
      "Epoch 4/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3590 - acc: 0.3105 - val_loss: 1.3479 - val_acc: 0.3278\n",
      "Test accuracy: 0.32778408378832485\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 2s - loss: 1.3826 - acc: 0.2684 - val_loss: 1.3667 - val_acc: 0.3207\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.3743 - acc: 0.2839 - val_loss: 1.3663 - val_acc: 0.3162\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 1s - loss: 1.3703 - acc: 0.2961 - val_loss: 1.3691 - val_acc: 0.3111\n",
      "Test accuracy: 0.31113633495033244\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.6593 - acc: 0.2522 - val_loss: 1.3859 - val_acc: 0.2588\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 0s - loss: 1.3867 - acc: 0.2557 - val_loss: 1.3859 - val_acc: 0.2588\n",
      "Test accuracy: 0.25879682178214064\n",
      "(31715, 14) (31715, 4) (7929, 14) 4\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/10\n",
      "31715/31715 [==============================] - 4s - loss: 1.3727 - acc: 0.2958 - val_loss: 1.3474 - val_acc: 0.3394\n",
      "Epoch 2/10\n",
      "31715/31715 [==============================] - 3s - loss: 1.3472 - acc: 0.3254 - val_loss: 1.3434 - val_acc: 0.3182\n",
      "Epoch 3/10\n",
      "31715/31715 [==============================] - 3s - loss: 1.3462 - acc: 0.3286 - val_loss: 1.3397 - val_acc: 0.3437\n",
      "Epoch 4/10\n",
      "31715/31715 [==============================] - 3s - loss: 1.3438 - acc: 0.3311 - val_loss: 1.3410 - val_acc: 0.3401\n",
      "Test accuracy: 0.34014377605721124\n",
      "Evalutation of best performing model:\n",
      "5568/7929 [====================>.........] - ETA: 0s[1.3409599813751651, 0.34014377605721124]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Dense': 0, 'Dense_1': 1, 'Dense_2': 2, 'Dense_3': 2, 'Dropout': 0.4143619965361732, 'Dropout_1': 0.09225974322037533, 'Dropout_2': 0.20942239619394942, 'batch_size': 0, 'optimizer': 1}\n"
     ]
    }
   ],
   "source": [
    "#Running hyper-param optimalization and choosing the best model\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=10,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='fully_connected_multi')\n",
    "X_train, X_test, y_train, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(np.array(X_test), np.array(y_test)))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save best model\n",
    "def save_model(model,model_name, weights_name):\n",
    "    model_json = model.to_json()\n",
    "    with open(model_name, \"w\") as f:\n",
    "        f.write(model_json)\n",
    "    model.save_weights(weights_name)\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "save_model(best_model, 'models/multi_model.json', 'models/multi_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19837</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.047722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_channel_is_socmed   data_channel_is_tech   data_channel_is_bus  \\\n",
       "19837                      0.0                    0.0                   1.0   \n",
       "\n",
       "        data_channel_is_world   data_channel_is_entertainment   is_weekend  \\\n",
       "19837                     0.0                             0.0          0.0   \n",
       "\n",
       "        weekday_is_friday   weekday_is_monday   weekday_is_tuesday  \\\n",
       "19837                 0.0                 0.0                  1.0   \n",
       "\n",
       "        weekday_is_thursday   weekday_is_wednesday   title_sentiment_polarity  \\\n",
       "19837                   0.0                    0.0                   0.333333   \n",
       "\n",
       "        n_tokens_title   global_sentiment_polarity  \n",
       "19837             15.0                    0.047722  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(best_model.predict(np.array(X_train[0:1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'the label [1] is not in the [index]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_type\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m                     \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36merror\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1500\u001b[0m                                .format(key=key,\n\u001b[0;32m-> 1501\u001b[0;31m                                        axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'the label [1] is not in the [index]'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f589c8c628ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"row{}.json\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1627\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_type\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1512\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m                 \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36merror\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 raise KeyError(u\"the label [{key}] is not in the [{axis}]\"\n\u001b[1;32m   1500\u001b[0m                                .format(key=key,\n\u001b[0;32m-> 1501\u001b[0;31m                                        axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'the label [1] is not in the [index]'"
     ]
    }
   ],
   "source": [
    "print(X_train.loc[1].to_json(\"row{}.json\".format(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
